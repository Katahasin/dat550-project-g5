{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook for classsifying using LSTM and Neural  Netwrok.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMPORTS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,Conv1D,MaxPool1D,SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "from keras.losses import BinaryCrossentropy\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "import itertools\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Loading Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "status_paths = glob(\"../data/statuses/*.csv\")\n",
    "len(status_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse_status_csv(path):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        parse_dates=['status_published'],\n",
    "        date_parser=lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'),\n",
    "    ).assign(page_id=lambda x: x[\"status_id\"].str.split(\"_\").str[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_statuses = pd.concat([ parse_status_csv(path) for path in status_paths ])\\\n",
    "    .drop_duplicates(subset=[\"status_id\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sites = pd.read_csv('../data/all-partisan-sites.csv')\n",
    "domain = pd.read_csv('../data/domaintools-whois-results.csv')\n",
    "page_info = pd.read_csv('../data/pages-info.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking the size of the datasets\n",
    "print('The shape of the site data set is :',sites.shape)\n",
    "print('The shape of the status data set is :',_statuses.shape)\n",
    "print('The shape of the domain data set is :',domain.shape)\n",
    "print('The shape of the page_info data set is :',page_info.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_statuses.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sites.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_info.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "domain.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def unique_values(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    print(\"UNIQUE VALUES  OF DATAFRAME %s\" % name +' ARE:')\n",
    "    for col in df.columns:\n",
    "        print(\" {column}: {length} \".format(column =col, length = len(df[col].unique())))\n",
    "    print('\\n')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def column_type(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    print(\"THE DATA TYPES OF COLUMNS FOR DATAFRAME %s\" % name +' ARE:')\n",
    "    for col in df.columns:\n",
    "        print(\" {column}: {length} \".format(column =col, length = df[col].dtypes))\n",
    "    print('\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for dataset in [_statuses,domain,sites,page_info]:\n",
    "    unique_values(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for dataset in [_statuses,domain,sites,page_info]:\n",
    "    column_type(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Getting some statistic information about political_category \n",
    "print(sites['political_category'].describe()) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def checking_nan(df):\n",
    "    missing_values=df.isnull().sum().sort_values(ascending=False)\n",
    "    percent_nan = (df.isnull().sum() / df.isnull().count()*100).sort_values(ascending=False)\n",
    "    nan_values = pd.concat([missing_values, percent_nan], axis=1, keys=['missing_values', 'Percent'])\n",
    "    nan_values=pd.DataFrame(nan_values)\n",
    "    return nan_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def droping_nan(df,thre):\n",
    "    # Dropping columns that having more than thre% of the missing values\n",
    "    threshold=int(len(df)*thre)\n",
    "    df=df.dropna(axis=1,thresh=threshold)\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def checking_nan_after_removing(df):\n",
    "    # After removing the columns that has more than 70% null values, we check again which columns has NULL values\n",
    "    missing_values=df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_values=pd.DataFrame(df.isnull().sum().sort_values(ascending=False),columns=['missing_values'])\n",
    "    miss_val=missing_values[missing_values['missing_values']>0]\n",
    "    return miss_val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cheking nan_value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sites_nan=checking_nan(sites) # checking nan_values for sites dataset\n",
    "sites_nan"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_nan=checking_nan(page_info) # checking nan_values for page_info dataset\n",
    "page_nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_status_nan=checking_nan(_statuses) # checking nan_values for statuses dataset\n",
    "_status_nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "domain_nan=checking_nan(domain) # checking nan_values for domain_ dataset\n",
    "domain_nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropping all column more than a particular percent of nan_values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sites=droping_nan(sites,0.7)  # dropping all columns having more than 85% nan_values for sites data set\n",
    "sites.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_info=droping_nan(page_info,0.7)  # dropping all columns having more than 85% nan_values page data set\n",
    "page_info.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "domain=droping_nan(domain,0.7)  # dropping all columns having more than 85% nan_values domain data set\n",
    "domain.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cheking how many NAN-values we still have after removing some"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sites_after_rm_nan=checking_nan_after_removing(sites) # Checking if there is still any nan values in dataframe\n",
    "sites_after_rm_nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Dispalying sites dataframe with its corresponding nan \n",
    "sites_nan_rows=sites[sites.isna().any(axis=1)]\n",
    "sites_nan_rows\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_after_rm_nan=checking_nan_after_removing(page_info) \n",
    "page_after_rm_nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dispalying page_info dataframe with its corresponding nan \n",
    "page_nan_rows=page_info[page_info.isna().any(axis=1)]\n",
    "page_nan_rows.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "domain_after_rm_nan=checking_nan_after_removing(domain) \n",
    "domain_after_rm_nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_url(text):\n",
    "    text = str(text).replace('www.', '').replace('http://', '')\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_info['website'] = page_info['website'].apply(remove_url).str.lower() # removing http://www.from page_info website\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_info[\"page_id\"] = page_info[\"page_id\"].astype(int).astype(str) # Converting page_id type from int64 to object to get datasets merged\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_site_page=pd.merge(sites, page_info, left_on=\"fb_id\", right_on=\"page_id\") # Combining site and page dataset based on Fb_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_status_sit_page=pd.merge(_statuses, merged_site_page, left_on=\"page_id\", right_on=\"page_id\") # combining merged_site_page and statuses  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_data=merged_status_sit_page.join( merged_status_sit_page['status_id'].str.split('_', expand=True).rename(columns={0:'fb_ID', 1:'status_ID'}))\n",
    "merged_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_data.isnull().sum() # Checking if there is any NAN-values in merged data set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_data.drop(['fb_ID', 'link_name','page_id','site','about','status_id','status_link'], axis = 1,inplace=True) # removing unnecessary columns\n",
    "merged_data.rename(columns = {'status_ID':'status_id'}, inplace = True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cols_to_move = ['status_id', 'fb_id','status_message','political_category','macedonian','website']\n",
    "merged_data= merged_data[ cols_to_move + [ col for col in merged_data.columns if col not in cols_to_move ] ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_data.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_data.dropna(subset=['status_message'],axis=0,inplace=True)  # Removing null values in 'about' columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_data=merged_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def text_cleaning(text):\n",
    "    word= BeautifulSoup(text, 'html.parser').get_text()\n",
    "    word= re.sub(r'https?://\\S+|www\\.\\S+', '', word)\n",
    "    word= unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    word=re.sub(r'[^a-zA-Z]', ' ', word)\n",
    "    word=re.sub(r'^\\s*|\\s\\s*', ' ', word).strip()\n",
    "    word=''.join(ch for ch, _ in itertools.groupby(word))\n",
    "    word = (re.compile(r'\\b(' + r'|'.join(stopwords.words(\"english\")) + r')\\b\\s*')).sub('', word).lower()\n",
    "    word=(re.sub('([A-Z])', r' \\1', word)).replace('  ',' ')\n",
    "\n",
    "    return word"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_data['status_message']=final_data['status_message'].apply(text_cleaning) # Cleaning the text data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We need numerical values in ML, therefore we convert political_category variable to a numeric variable\n",
    "final_data['political_category'] = final_data['political_category'].map({'right':1 ,'left':0})  \n",
    "  \n",
    "final_data[\"fb_id\"] = final_data[\"fb_id\"].astype(str).astype(int) # Converting fb_column from object type to int64\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_data['political_category'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_data['macedonian'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x='macedonian',data=final_data)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x='political_category',data=final_data)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "status_messages = ' '.join(map(str,page_info['about']))\n",
    "\n",
    "wordcloud=WordCloud(width=1920,height=1080).generate(status_messages)\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2vec preproccesing using CBOW and Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def X_tokenizer_vocab(x, max_length):\n",
    "   \n",
    "    tokenizer= Tokenizer()  # This class allows to vectorize a text corpus, by turning each text into a sequence of integers \n",
    "    tokenizer.fit_on_texts(x) #  A list of sequence. A \"sequence\" is a list of integer word indices.\n",
    "    X = tokenizer.texts_to_sequences(x)  # By this code we get a list of sequence I.e. for each particular word a sequence number is assigned. \n",
    "    X= pad_sequences(X, maxlen=max_length) #  every sequence are of max_length\n",
    "    vocab_size=len(tokenizer.word_index)+1  # The length of vocabularies in the text data. This is the input of LSTM model and column size of Embedding matrix.\n",
    "    vocab= tokenizer.word_index # Vocabularies in text data used in Embedding matrix.\n",
    "\n",
    "    return X,vocab_size,vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# function for creating Embedding Matrix\n",
    "def get_weight_matrix(model,weight_matrix_rows,weight_matrix_rows_column,vocabulary):\n",
    "    weight_matrix= np.zeros((weight_matrix_rows,weight_matrix_rows_column))\n",
    "    \n",
    "    for word,i in vocabulary.items():\n",
    "        weight_matrix[i] = model.wv[word]\n",
    "    return weight_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df=final_data.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x= [d.split() for d in df['status_message'].tolist()] # We get our text data in the form list of words\n",
    "x= [i for i in x if i != []]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X,vocab_size, vocab=X_tokenizer_vocab(x, max_length=40) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y=df['political_category'].values\n",
    "y=y[:len(X)]\n",
    "len(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DIM=100\n",
    "max_length=40\n",
    "w2v_model_cbow=gensim.models.Word2Vec(sentences=x,vector_size=DIM,window=10,min_count=1,sg=0) # Creating Word2vect model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(w2v_model_cbow.wv) # lengh of created unique word"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w2v_model_cbow.wv.key_to_index "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w2v_model_cbow.wv # By executing this model our text data is  converteted to a set of vector "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w2v_model_cbow.wv['us']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w2v_model_cbow.wv.most_similar('us') # Most similiar word for a specific letter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding_vectors= get_weight_matrix(w2v_model_cbow,vocab_size,DIM,vocab)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding_vectors.shape # rows are the lenght of created unique word and columns is size of  dim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y) # Using SKit-learn libirary to split our data into training data and test data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('The shape of the X_train is :',X_train.shape)\n",
    "print('The shape of the X_test is :',X_test.shape)\n",
    "print('The shape of the y_train is :',y_train.shape)\n",
    "print('The shape of the y_test is :',y_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating ML model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model(classifier=''):\n",
    "    model= Sequential()\n",
    "\n",
    "    if classifier=='LSTM':\n",
    "        model= Sequential()\n",
    "        model.add(Embedding(vocab_size,output_dim=DIM,weights=[embedding_vectors],input_length=max_length,trainable=False))\n",
    "        model.add(LSTM(units=128))\n",
    "        model.add(Dense(32,activation='relu'))\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "    else:\n",
    "        model.add(Dense(30,activation='relu',input_dim=X_train.shape[1]))\n",
    "        model.add(Dense(60,activation='sigmoid'))\n",
    "        model.add(Dense(60,activation='relu'))\n",
    "        model.add(Dense(20,activation='tanh'))\n",
    "        model.add(Dense(70,activation='tanh'))\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        model.compile(optimizer= tensorflow.keras.optimizers.Adam(learning_rate=0.001),loss=BinaryCrossentropy(from_logits=False),metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ML model uisng LSTM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LSTM_model=model(classifier='LSTM') # Model created using LSTM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LSTM_model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LSTM_model.fit(X_train,y_train,validation_split=0.3,epochs=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train_pred_lstm= (LSTM_model.predict(X_train)>=0.5).astype(int)\n",
    "print( 'LSTM got the following accuracy on the train set',accuracy_score(y_train,y_train_pred_lstm))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_test_pred_lstm= (LSTM_model.predict(X_test)>=0.5).astype(int)\n",
    "print( 'LSTM got the following accuracy on the test set',accuracy_score(y_test,y_test_pred_lstm))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Netwok "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NN_model=model(classifier='NN')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NN_model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NN_model.fit(X_train,y_train,validation_split=0.3,epochs=5)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train_pred_NN= (NN_model.predict(X_train)>=0.5).astype(int)\n",
    "print( 'NN got the following accuracy on the train set',accuracy_score(y_train,y_train_pred_NN))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_test_pred_NN= (NN_model.predict(X_test)>=0.5).astype(int)\n",
    "print( 'NN got the following accuracy on the test set',accuracy_score(y_test,y_test_pred_NN))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f83f964ac515f2130cd88cd8cac989186b3aa144b7937081036ff030079bc1d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}